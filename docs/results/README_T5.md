# Fine-tuning Results for T5-base

This report presents the evaluation metrics for the T5-base model fine-tuned on three tasks: Question Answering, Translation, and Sentiment Classification. Each task is evaluated under three fine-tuning methods: Full Finetuning, LoRA, and Adapters.

---

## Task 1: Question Answering  
**Dataset:** [SQuAD 1.0](https://rajpurkar.github.io/SQuAD-explorer/)  
**Metrics:** Exact Match (EM), F1 Score

| Method           | Exact Match (EM) | F1 Score |
|------------------|------------------|----------|
| Full Finetuning  |                  |          |
| LoRA             |                  |          |
| Adapters         |                  |          |

---

## Task 2: Translation  
**Dataset:** [WMT16 English-German](https://huggingface.co/datasets/wmt/wmt16)  
**Metrics:** BLEU Score

| Method           | BLEU Score |
|------------------|------------|
| Full Finetuning  |            |
| LoRA             |            |
| Adapters         |            |

---

## Task 3: Sentiment Classification  
**Dataset:** [IMDB](https://huggingface.co/datasets/stanfordnlp/imdb)  
**Metrics:** Accuracy, Precision, Recall, F1 Score

| Method           | Accuracy | Precision | Recall | F1 Score |
|------------------|----------|-----------|--------|----------|
| Full Finetuning  |          |           |        |          |
| LoRA             |          |           |        |          |
| Adapters         |          |           |        |          |
