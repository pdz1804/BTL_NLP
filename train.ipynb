{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b879e035",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac90c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380bb35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For finetuning\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f156c8",
   "metadata": {},
   "source": [
    "# Configure Finetune Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434be596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of implemented methods\n",
    "models   = ['t5-base', 'bart-base', 'prophetnet-large-uncased']\n",
    "datasets = ['squad', 'wmt16_en_de', 'imdb']\n",
    "finetunes = ['full', 'lora', 'adapters']\n",
    "\n",
    "# Selecting index\n",
    "model, dataset, finetune = 0, 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e25b7",
   "metadata": {},
   "source": [
    "## Saved Directory for Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8432614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = {\n",
    "    \"squad\": \"qa\",\n",
    "    \"wmt16_en_de\": \"translation\",\n",
    "    \"imdb\": \"textsentiment\"\n",
    "}\n",
    "\n",
    "model_path = f'models/ft-{models[model]}-{finetunes[finetune]}-{task[datasets[dataset]]}'\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb03690",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab12cc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure training\n",
    "num_train_epochs = 1\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 0.02\n",
    "logging_steps = 1\n",
    "\n",
    "# reduce if CUDA Out Of Memory\n",
    "train_batch_size = 1\n",
    "eval_batch_size = 1\n",
    "\n",
    "# turn into `False' for full training\n",
    "test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48019b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if models[model] == 'prophetnet-large-uncased':\n",
    "    device = torch.device('cpu') # manually setup for prophetnet since it is too large\n",
    "else:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'Using device: {device}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf3747",
   "metadata": {},
   "source": [
    "# Setup Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b8d956",
   "metadata": {},
   "source": [
    "Wandb keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b4f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('api_key.json', 'r', encoding='utf-8') as file:\n",
    "    api_keys = json.load(file)\n",
    "\n",
    "WANDB_TOKEN, WANDB_API = api_keys['hf_token'], api_keys['hf_api']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5f6aaf",
   "metadata": {},
   "source": [
    "Start trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647bd7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import trainers pipeline\n",
    "from modules.trainer import BaseTrainer\n",
    "\n",
    "\"\"\"\n",
    "Args:\n",
    "    device (torch.device): device used for finetuning.\n",
    "    model (str): name of the model.\n",
    "    dataset (str): name of the dataset.\n",
    "    finetune (str): name of the finetune strategy.\n",
    "\"\"\"\n",
    "\n",
    "# Configure\n",
    "trainer = BaseTrainer(\n",
    "    device=device,\n",
    "    model=models[model],\n",
    "    dataset=datasets[dataset],\n",
    "    finetune=finetunes[finetune],\n",
    "    train_batch_size=train_batch_size,\n",
    "    eval_batch_size=eval_batch_size,\n",
    "    test=test\n",
    ")\n",
    "\n",
    "# Set up api key\n",
    "trainer.set_wandb_api(wandb_token=WANDB_TOKEN, wandb_api=WANDB_API, project='phat-ft-nlp-test')\n",
    "\n",
    "# Start training loop\n",
    "trainer.run(\n",
    "    saved_model = model_path,\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    learning_rate = learning_rate,\n",
    "    weight_decay = weight_decay\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
